# Linear Transformations

> **Linear transformations are the mathematical foundation for how neural networks process and transform data.**

---

## What is a Linear Transformation?

A **linear transformation** is a function $T: \mathbb{R}^n \to \mathbb{R}^m$ that satisfies two properties for all vectors $\mathbf{u}, \mathbf{v}$ and all scalars $c$:

1. **Additivity:**
   $`
   T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})
   `$ 
2. **Homogeneity (Scalar Multiplication):**
   $`
   T(c\mathbf{u}) = cT(\mathbf{u})
   `$ 

**Step-by-step:**
- Additivity: The transformation of a sum is the sum of the transformations.
- Homogeneity: The transformation of a scaled vector is the scaled transformation.

**Intuition:**
- A linear transformation preserves vector addition and scalar multiplication.
- It always maps the origin to the origin.
- Examples: rotations, scalings, reflections, projections.
- Non-examples: translations, nonlinear warps.

> **Tip:** If a transformation doesn't preserve the origin, it's not linear!

Every linear transformation can be represented as a matrix multiplication.

---

## Matrix Representation

If $T$ is a linear transformation, there exists a matrix $A$ such that:

```math
T(\mathbf{x}) = A\mathbf{x}
```

This means that applying a linear transformation is the same as multiplying by a matrix.

**Step-by-step:**
- Write the transformation as a matrix $A$.
- Multiply $A$ by the vector $\mathbf{x}$ to get the transformed vector.

**Example:**
If $A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$ and $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$, then $A\mathbf{x} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$ (scaling $x$ by 2 and $y$ by 3).

> **Pitfall:** Not every function is linear! Only those that can be written as $A\mathbf{x}$.

---

## Common Linear Transformations in 2D

### Rotation

Rotation by angle $\theta$:

```math
R(\theta) = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix}
```

**Step-by-step:**
- Multiply the vector by this matrix to rotate it by $\theta$ radians counterclockwise.

### Scaling

Scaling by $s_x$ and $s_y$:

```math
S = \begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix}
```

**Step-by-step:**
- Multiplies the $x$-component by $s_x$ and the $y$-component by $s_y$.

### Shear

Horizontal shear by $k$:

```math
H = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}
```

**Step-by-step:**
- Adds $k$ times the $y$-component to the $x$-component.

**Other examples:**
- **Reflection:** $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ reflects across the $x$-axis.
- **Projection:** $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ projects onto the $x$-axis.

---

## Geometric Interpretation

- **Rotation** changes the direction of vectors but not their length (if $s_x = s_y = 1$).
- **Scaling** changes the length of vectors (stretches or shrinks).
- **Shear** skews the shape of objects (slants rectangles into parallelograms).
- **Reflection** flips vectors over a line.
- **Projection** "flattens" vectors onto a subspace.

**Visualization:**
- Imagine a grid of points in 2D. A linear transformation moves every point, but straight lines remain straight, and the origin stays fixed.

> **Tip:** Linear transformations always map lines to lines and preserve the origin.

---

## Python Implementation: Visualizing Transformations

Let's visualize how these transformations affect a simple shape (a square):

```python
import numpy as np
import matplotlib.pyplot as plt

# Define a square (5 points to close the shape)
square = np.array([[0, 1, 1, 0, 0],
                   [0, 0, 1, 1, 0]])

# Rotation (45 degrees)
theta = np.pi / 4
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta), np.cos(theta)]])
rotated_square = R @ square  # Rotate each point

# Scaling (2x in x, 1.5x in y)
S = np.array([[2, 0], [0, 1.5]])
scaled_square = S @ square  # Scale each point

# Shear (k=1)
k = 1
H = np.array([[1, k], [0, 1]])
sheared_square = H @ square  # Shear each point

# Reflection over x-axis
F = np.array([[1, 0], [0, -1]])
reflected_square = F @ square  # Reflect each point

# Projection onto x-axis
P = np.array([[1, 0], [0, 0]])
projected_square = P @ square  # Project each point

# Plot
plt.figure(figsize=(20, 4))

plt.subplot(1, 5, 1)
plt.plot(square[0], square[1], 'b-', linewidth=2)
plt.title('Original Square')
plt.axis('equal'); plt.grid(True)

plt.subplot(1, 5, 2)
plt.plot(rotated_square[0], rotated_square[1], 'r-', linewidth=2)
plt.title('Rotated (45Â°)')
plt.axis('equal'); plt.grid(True)

plt.subplot(1, 5, 3)
plt.plot(scaled_square[0], scaled_square[1], 'g-', linewidth=2)
plt.title('Scaled (2x, 1.5y)')
plt.axis('equal'); plt.grid(True)

plt.subplot(1, 5, 4)
plt.plot(sheared_square[0], sheared_square[1], 'm-', linewidth=2)
plt.title('Sheared (k=1)')
plt.axis('equal'); plt.grid(True)

plt.subplot(1, 5, 5)
plt.plot(reflected_square[0], reflected_square[1], 'c-', linewidth=2)
plt.title('Reflected (x-axis)')
plt.axis('equal'); plt.grid(True)

plt.tight_layout()
plt.show()
```

**Code Annotations:**
- `np.array` creates the square as a set of points.
- Each transformation matrix (R, S, H, F, P) is applied to all points in the square.
- The `@` operator performs matrix multiplication.
- Each subplot shows the effect of a different linear transformation.
- `plt.axis('equal')` ensures the aspect ratio is preserved.

> **Tip:** Try changing the transformation matrices or the shape to see different effects!

---

## Why Linear Transformations Matter in Deep Learning

- **Neural network layers**: Each layer applies a linear transformation to its input (followed by a nonlinearity).
- **Feature extraction**: Transforming data to new spaces for better learning (e.g., PCA, learned embeddings).
- **Data augmentation**: Rotations, scalings, and shears are used to augment image data.
- **Understanding learned representations**: The weights of a neural network layer define a linear transformation of the input space.

> **Summary:** Understanding linear transformations helps you see how neural networks manipulate and learn from data! They are the mathematical backbone of all neural network computations. 