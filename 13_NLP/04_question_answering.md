# Question Answering (QA)

Question Answering (QA) is a task in NLP where a system is required to provide answers to questions posed in natural language. Modern QA systems leverage large pre-trained language models and can be categorized as extractive or generative.

> **Learning Objective:** By the end of this guide, you'll understand different types of question answering, how to implement extractive and generative QA systems, and how to evaluate their performance.

---

## 1. Types of Question Answering

- **Extractive QA:** The answer is a span of text extracted from a given context.
- **Generative QA:** The answer is generated by the model, possibly synthesizing information from multiple sources.

> **Key Insight:** Extractive QA is common in reading comprehension datasets (e.g., SQuAD), while generative QA is used in open-domain settings and conversational agents.

**Deep Dive: QA Categories**

**Extractive QA:**
- **Reading Comprehension:** Answer questions about a given passage
- **Span Extraction:** Find exact text spans as answers
- **Examples:** SQuAD, NewsQA, HotpotQA
- **Advantages:** Factual, verifiable, fast inference
- **Limitations:** Limited to text in context, can't synthesize information

**Generative QA:**
- **Open-Domain:** Answer questions using knowledge from multiple sources
- **Free-Form Answers:** Generate natural language responses
- **Examples:** Natural Questions, MS MARCO, conversational QA
- **Advantages:** More natural answers, can synthesize information
- **Limitations:** May hallucinate, harder to verify, slower inference

**Hybrid Approaches:**
- **Retrieve-then-Generate:** First retrieve relevant documents, then generate answer
- **Multi-Hop QA:** Combine information from multiple sources
- **Conversational QA:** Maintain context across multiple questions

**Common Pitfall:** Don't assume extractive QA is always better - choose based on your use case and requirements.

---

## 2. Extractive QA: Span Prediction

Given a context $C$ and a question $Q$, the model predicts the start and end positions $(s, e)$ of the answer span in $C$.

```math
(s^*, e^*) = \underset{(s, e)}{\arg\max}\; P(s, e \mid C, Q)
```

**Mathematical Intuition:**

1. **Joint Probability:** Model predicts start and end positions together
   ```math
   P(s, e \mid C, Q) = P(s \mid C, Q) \times P(e \mid s, C, Q)
   ```

2. **Span Constraints:** End position must come after start position
   ```math
   s \leq e \leq |C|
   ```

3. **Maximum Likelihood:** Find the most probable span
   ```math
   (s^*, e^*) = \arg\max_{s,e} P(s, e \mid C, Q)
   ```

**Model Architecture:**
- **Encoder:** Processes question and context (BERT, RoBERTa, etc.)
- **Start/End Heads:** Linear layers to predict start and end probabilities
- **Span Extraction:** Extract text between predicted positions

**Why Span Prediction Works:**
- **Natural Language Structure:** Answers are often contiguous text spans
- **Supervised Learning:** Can use existing QA datasets for training
- **Interpretable:** Easy to understand and verify answers
- **Efficient:** Fast inference compared to generation

**Challenges:**
- **Span Limitations:** Can't answer questions requiring synthesis
- **Context Length:** Limited by model's maximum input length
- **Ambiguous Spans:** Multiple valid answer spans possible

**🔧 Implementation Concept:**
```python
def extractive_qa_predict(model, tokenizer, question, context):
    """
    Predict answer span for extractive QA
    
    Args:
        model: QA model (e.g., BERT for QA)
        tokenizer: Tokenizer for the model
        question: Input question
        context: Context passage
    """
    # Tokenize input
    inputs = tokenizer(
        question, 
        context, 
        return_tensors='pt',
        max_length=512,
        truncation=True,
        padding=True
    )
    
    # Get model predictions
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits
    
    # Find best span
    start_probs = torch.softmax(start_logits, dim=-1)
    end_probs = torch.softmax(end_logits, dim=-1)
    
    # Get most probable start and end positions
    start_pos = torch.argmax(start_probs, dim=-1).item()
    end_pos = torch.argmax(end_probs, dim=-1).item()
    
    # Extract answer span
    answer_tokens = inputs['input_ids'][0][start_pos:end_pos+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    return answer, start_pos, end_pos
```

---

### Example: Using BERT for Extractive QA

```python
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

# Load pre-trained model and tokenizer
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"

inputs = tokenizer(question, context, return_tensors='pt')  # Tokenize question and context
outputs = model(**inputs)  # Get start and end logits

answer_start = torch.argmax(outputs.start_logits)  # Most probable start position
answer_end = torch.argmax(outputs.end_logits) + 1  # Most probable end position (+1 for inclusive slicing)
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))  # Convert token IDs to string
print("Answer:", answer)
```

**🔍 Detailed Code Walkthrough:**

```python
# Step 1: Import and Setup
from transformers import BertForQuestionAnswering, BertTokenizer
import torch
import numpy as np

# Step 2: Load Model and Tokenizer
# Available models: 'bert-base-uncased', 'bert-large-uncased', 'roberta-base', etc.
model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
model = BertForQuestionAnswering.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

# Step 3: Prepare Input Data
context = """
Deep learning is a subset of machine learning that uses artificial neural networks 
to model and understand complex patterns in data. It enables computers to learn 
from examples without being explicitly programmed for specific tasks.
"""

question = "What does deep learning enable?"

# Step 4: Tokenize Input
# Tokenizer combines question and context with special tokens
inputs = tokenizer(
    question,                    # Question text
    context,                     # Context text
    return_tensors='pt',         # Return PyTorch tensors
    max_length=512,              # Maximum sequence length
    truncation=True,             # Truncate if too long
    padding=True,                # Pad sequences
    return_overflowing_tokens=True,  # Handle long contexts
    stride=128                   # Overlap between chunks
)

# Step 5: Model Inference
with torch.no_grad():  # Disable gradient computation
    outputs = model(**inputs)
    
    # Get start and end logits for each token
    start_logits = outputs.start_logits  # Shape: (batch_size, sequence_length)
    end_logits = outputs.end_logits      # Shape: (batch_size, sequence_length)

# Step 6: Find Best Answer Span
def find_best_span(start_logits, end_logits, max_answer_length=30):
    """Find the best answer span given start and end logits"""
    
    # Get probabilities
    start_probs = torch.softmax(start_logits, dim=-1)
    end_probs = torch.softmax(end_logits, dim=-1)
    
    # Find valid spans (end >= start, within max length)
    best_score = float('-inf')
    best_start = 0
    best_end = 0
    
    for start_idx in range(start_logits.size(1)):
        for end_idx in range(start_idx, min(start_idx + max_answer_length, end_logits.size(1))):
            score = start_logits[0, start_idx] + end_logits[0, end_idx]
            if score > best_score:
                best_score = score
                best_start = start_idx
                best_end = end_idx
    
    return best_start, best_end

# Find best span
start_pos, end_pos = find_best_span(start_logits, end_logits)

# Step 7: Extract Answer
# Convert token IDs back to text
answer_tokens = inputs['input_ids'][0][start_pos:end_pos+1]
answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

print(f"Question: {question}")
print(f"Answer: {answer}")
print(f"Confidence: Start={start_logits[0, start_pos]:.2f}, End={end_logits[0, end_pos]:.2f}")
```

**Try It Yourself:**
```python
# Create a simple QA function
def ask_question(question, context, model, tokenizer):
    """Ask a question about a given context"""
    
    # Tokenize
    inputs = tokenizer(
        question, 
        context, 
        return_tensors='pt',
        max_length=512,
        truncation=True
    )
    
    # Get predictions
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits
    
    # Find best span
    start_pos = torch.argmax(start_logits, dim=-1).item()
    end_pos = torch.argmax(end_logits, dim=-1).item()
    
    # Extract answer
    answer_tokens = inputs['input_ids'][0][start_pos:end_pos+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    return answer

# Test with different questions
test_context = """
Artificial intelligence (AI) is the simulation of human intelligence in machines. 
Machine learning is a subset of AI that enables computers to learn and improve 
from experience without being explicitly programmed. Deep learning is a type of 
machine learning that uses neural networks with multiple layers.
"""

questions = [
    "What is artificial intelligence?",
    "How does machine learning work?",
    "What is deep learning?",
    "What are neural networks used for?"
]

for question in questions:
    answer = ask_question(question, test_context, model, tokenizer)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

**Advanced Extractive QA:**
```python
# Handle long contexts with sliding window
def extractive_qa_long_context(question, context, model, tokenizer, max_length=512, stride=128):
    """Handle long contexts by using sliding window approach"""
    
    # Split context into overlapping chunks
    tokens = tokenizer.tokenize(context)
    chunks = []
    
    for i in range(0, len(tokens), max_length - stride):
        chunk = tokens[i:i + max_length]
        chunks.append(chunk)
    
    best_answer = ""
    best_score = float('-inf')
    
    for chunk in chunks:
        # Tokenize chunk
        chunk_text = tokenizer.convert_tokens_to_string(chunk)
        inputs = tokenizer(question, chunk_text, return_tensors='pt', max_length=max_length, truncation=True)
        
        # Get predictions
        with torch.no_grad():
            outputs = model(**inputs)
            start_logits = outputs.start_logits
            end_logits = outputs.end_logits
        
        # Find best span in this chunk
        start_pos = torch.argmax(start_logits, dim=-1).item()
        end_pos = torch.argmax(end_logits, dim=-1).item()
        
        # Calculate score
        score = start_logits[0, start_pos] + end_logits[0, end_pos]
        
        if score > best_score:
            best_score = score
            answer_tokens = inputs['input_ids'][0][start_pos:end_pos+1]
            best_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    return best_answer, best_score

# Test with long context
long_context = """
[Long document content here...]
"""
answer, score = extractive_qa_long_context("Your question", long_context, model, tokenizer)
print(f"Answer: {answer}")
print(f"Score: {score:.2f}")
```

> **Pro Tip:** For better results, preprocess your context to remove irrelevant information and ensure the answer is actually present in the text.

---

## 3. Generative QA

Generative models (e.g., T5, GPT) generate answers in free-form text, not limited to spans in the context.

**Key Differences from Extractive QA:**

1. **Output Format:** Free-form text instead of text spans
2. **Training Objective:** Sequence-to-sequence learning
3. **Answer Quality:** Can synthesize information from multiple sources
4. **Flexibility:** Can answer questions not directly answered in context

**Why Generative QA Works:**
- **Knowledge Synthesis:** Can combine information from multiple sources
- **Natural Language:** Generates more natural-sounding answers
- **Open-Domain:** Can answer questions beyond the given context
- **Conversational:** Better for dialogue systems

**Challenges:**
- **Hallucination:** May generate incorrect information
- **Verification:** Harder to verify answer accuracy
- **Training Data:** Requires more diverse training data
- **Computational Cost:** Slower inference than extractive QA

** Implementation Concept:**
```python
def generative_qa_predict(model, tokenizer, question, context=None):
    """
    Generate answer using generative QA model
    
    Args:
        model: Generative model (e.g., T5, GPT)
        tokenizer: Tokenizer for the model
        question: Input question
        context: Optional context (for some models)
    """
    # Format input based on model type
    if context:
        input_text = f"question: {question} context: {context}"
    else:
        input_text = f"question: {question}"
    
    # Tokenize input
    inputs = tokenizer(
        input_text,
        return_tensors='pt',
        max_length=512,
        truncation=True,
        padding=True
    )
    
    # Generate answer
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=128,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=2
        )
    
    # Decode answer
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer
```

---

### Example: Using T5 for Generative QA

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained('t5-base')  # Load T5 model

tokenizer = T5Tokenizer.from_pretrained('t5-base')  # Load tokenizer

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"
input_text = f"question: {question}  context: {context}"
input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Encode input as tensor
outputs = model.generate(input_ids)  # Generate answer
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Decode to text
print("Answer:", answer)
```

**🔍 Detailed Code Walkthrough:**

```python
# Step 1: Import and Setup
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch

# Step 2: Load Model and Tokenizer
# Available models: 't5-small', 't5-base', 't5-large', 't5-3b', 't5-11b'
model_name = 't5-base'
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

# Step 3: Prepare Input Data
context = """
Machine learning is a subset of artificial intelligence that enables computers 
to learn and improve from experience without being explicitly programmed. 
Deep learning is a type of machine learning that uses neural networks with 
multiple layers to model complex patterns in data.
"""

question = "How does deep learning work?"

# Step 4: Format Input
# T5 uses specific input formats for different tasks
input_text = f"question: {question} context: {context}"

# Step 5: Tokenize Input
inputs = tokenizer(
    input_text,
    return_tensors='pt',
    max_length=512,      # Maximum input length
    truncation=True,     # Truncate if too long
    padding=True         # Pad sequences
)

# Step 6: Generate Answer
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=128,           # Maximum output length
        num_beams=4,             # Beam search with 4 beams
        early_stopping=True,      # Stop when all beams reach EOS
        no_repeat_ngram_size=2,   # Avoid repeating 2-grams
        temperature=0.7,          # Control randomness
        do_sample=True,           # Use sampling
        top_k=50,                # Top-k sampling
        top_p=0.9                # Nucleus sampling
    )

# Step 7: Decode Answer
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Question: {question}")
print(f"Answer: {answer}")
```

**Try It Yourself:**
```python
# Create a generative QA function
def generate_answer(question, context=None, model_name='t5-base'):
    """Generate answer using T5 model"""
    
    # Load model and tokenizer
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    
    # Format input
    if context:
        input_text = f"question: {question} context: {context}"
    else:
        input_text = f"question: {question}"
    
    # Tokenize and generate
    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=128,
            num_beams=4,
            early_stopping=True
        )
    
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# Test different types of questions
questions = [
    "What is the capital of France?",
    "How do plants make food?",
    "What causes climate change?",
    "Who wrote Romeo and Juliet?"
]

for question in questions:
    answer = generate_answer(question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

**Advanced Generative QA:**
```python
# Multi-turn conversational QA
def conversational_qa(questions, context=None):
    """Handle multi-turn questions in a conversation"""
    
    model = T5ForConditionalGeneration.from_pretrained('t5-base')
    tokenizer = T5Tokenizer.from_pretrained('t5-base')
    
    conversation_history = []
    answers = []
    
    for i, question in enumerate(questions):
        # Build conversation context
        if conversation_history:
            conv_context = " ".join([f"Q: {q} A: {a}" for q, a in conversation_history])
            if context:
                full_context = f"{context} {conv_context}"
            else:
                full_context = conv_context
        else:
            full_context = context or ""
        
        # Format input
        input_text = f"question: {question} context: {full_context}"
        
        # Generate answer
        inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_length=128,
                num_beams=4,
                early_stopping=True
            )
        
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Update conversation history
        conversation_history.append((question, answer))
        answers.append(answer)
    
    return answers

# Test conversational QA
conversation = [
    "What is machine learning?",
    "How is it different from deep learning?",
    "What are some applications?"
]

answers = conversational_qa(conversation)
for q, a in zip(conversation, answers):
    print(f"Q: {q}")
    print(f"A: {a}\n")
```

---

## 4. Applications

**Search Engines:**
- **Direct Answers:** Provide answers directly in search results
- **Featured Snippets:** Extract relevant information from web pages
- **Knowledge Graph:** Answer factual questions using structured data

**Virtual Assistants:**
- **Factual Answers:** Provide accurate information to users
- **Task Completion:** Help users accomplish specific tasks
- **Conversational AI:** Maintain context across multiple questions

**Customer Support Bots:**
- **FAQ Automation:** Answer common customer questions
- **Documentation Search:** Find relevant information in manuals
- **Troubleshooting:** Help users solve problems

**Reading Comprehension:**
- **Educational Tools:** Help students understand text
- **Research Assistance:** Extract information from academic papers
- **Content Analysis:** Understand and summarize documents

** Example Application: FAQ Bot**
```python
class FAQBot:
    def __init__(self, faq_data, model_name='t5-base'):
        """Initialize FAQ bot with question-answer pairs"""
        self.faq_data = faq_data
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
    
    def find_best_match(self, user_question):
        """Find the most similar FAQ question"""
        best_match = None
        best_score = 0
        
        for faq_q, faq_a in self.faq_data.items():
            # Simple similarity (could use more sophisticated methods)
            similarity = self._calculate_similarity(user_question, faq_q)
            if similarity > best_score:
                best_score = similarity
                best_match = (faq_q, faq_a)
        
        return best_match, best_score
    
    def _calculate_similarity(self, q1, q2):
        """Calculate similarity between two questions"""
        # Simple word overlap (could use embeddings)
        words1 = set(q1.lower().split())
        words2 = set(q2.lower().split())
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        return len(intersection) / len(union) if union else 0
    
    def answer_question(self, user_question, threshold=0.3):
        """Answer user question using FAQ or generation"""
        
        # Try to find FAQ match
        best_match, score = self.find_best_match(user_question)
        
        if score > threshold and best_match:
            return best_match[1], "FAQ"
        else:
            # Generate answer
            input_text = f"question: {user_question}"
            inputs = self.tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    max_length=128,
                    num_beams=4,
                    early_stopping=True
                )
            
            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return answer, "Generated"

# Example usage
faq_data = {
    "What is your return policy?": "We offer 30-day returns for all products.",
    "How do I contact customer service?": "You can reach us at support@company.com or call 1-800-123-4567.",
    "Do you ship internationally?": "Yes, we ship to most countries worldwide."
}

bot = FAQBot(faq_data)

questions = [
    "What's your return policy?",
    "How can I get help?",
    "Do you ship to Canada?",
    "What's the weather like today?"
]

for question in questions:
    answer, source = bot.answer_question(question)
    print(f"Q: {question}")
    print(f"A: {answer} (Source: {source})\n")
```

**Example Application: Document QA System**
```python
class DocumentQASystem:
    def __init__(self, documents, model_name='t5-base'):
        """Initialize QA system with document collection"""
        self.documents = documents
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
    
    def search_relevant_docs(self, question, top_k=3):
        """Find most relevant documents for the question"""
        # Simple keyword matching (could use embeddings)
        relevant_docs = []
        
        for doc_id, doc_content in self.documents.items():
            score = self._calculate_relevance(question, doc_content)
            relevant_docs.append((doc_id, doc_content, score))
        
        # Sort by relevance and return top-k
        relevant_docs.sort(key=lambda x: x[2], reverse=True)
        return relevant_docs[:top_k]
    
    def _calculate_relevance(self, question, document):
        """Calculate relevance between question and document"""
        # Simple word overlap (could use semantic similarity)
        q_words = set(question.lower().split())
        d_words = set(document.lower().split())
        intersection = q_words.intersection(d_words)
        return len(intersection) / len(q_words) if q_words else 0
    
    def answer_question(self, question):
        """Answer question using relevant documents"""
        
        # Find relevant documents
        relevant_docs = self.search_relevant_docs(question)
        
        if not relevant_docs:
            return "I don't have enough information to answer this question."
        
        # Combine relevant documents as context
        context = " ".join([doc[1] for doc in relevant_docs])
        
        # Generate answer
        input_text = f"question: {question} context: {context}"
        inputs = self.tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=128,
                num_beams=4,
                early_stopping=True
            )
        
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return answer, [doc[0] for doc in relevant_docs]

# Example usage
documents = {
    "doc1": "Machine learning is a subset of artificial intelligence that enables computers to learn from data.",
    "doc2": "Deep learning uses neural networks with multiple layers to model complex patterns.",
    "doc3": "Natural language processing helps computers understand and generate human language."
}

qa_system = DocumentQASystem(documents)

questions = [
    "What is machine learning?",
    "How does deep learning work?",
    "What is NLP used for?"
]

for question in questions:
    answer, sources = qa_system.answer_question(question)
    print(f"Q: {question}")
    print(f"A: {answer}")
    print(f"Sources: {sources}\n")
```

---

## 5. Further Reading

**Foundational Papers:**
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) - BERT for QA
- [A Unified Approach to QA (T5, Raffel et al., 2020)](https://arxiv.org/abs/1910.10683) - T5 for multiple tasks

**🔬 Advanced Topics:**
- **Multi-Hop QA:** Answering questions requiring multiple reasoning steps
- **Conversational QA:** Maintaining context across multiple questions
- **Open-Domain QA:** Answering questions without given context
- **Evaluation Metrics:** EM, F1, BLEU, ROUGE for QA evaluation

**Next Steps:**
1. **Experiment with different models:** Try BERT, RoBERTa, T5, GPT for QA
2. **Learn evaluation metrics:** Understand how to measure QA performance
3. **Explore datasets:** Work with SQuAD, HotpotQA, Natural Questions
4. **Study advanced techniques:** Multi-hop reasoning, conversational QA

> **Explore these papers to learn more about the theory and practice of question answering!**

**Try It Yourself:**
Build a simple QA evaluation system:
```python
def evaluate_qa_system(model, tokenizer, test_data):
    """Evaluate QA system on test data"""
    
    correct = 0
    total = len(test_data)
    
    for question, context, gold_answer in test_data:
        # Get model prediction
        if isinstance(model, BertForQuestionAnswering):
            # Extractive QA
            inputs = tokenizer(question, context, return_tensors='pt')
            outputs = model(**inputs)
            start_pos = torch.argmax(outputs.start_logits, dim=-1).item()
            end_pos = torch.argmax(outputs.end_logits, dim=-1).item()
            predicted_answer = tokenizer.decode(
                inputs['input_ids'][0][start_pos:end_pos+1], 
                skip_special_tokens=True
            )
        else:
            # Generative QA
            input_text = f"question: {question} context: {context}"
            inputs = tokenizer(input_text, return_tensors='pt')
            outputs = model.generate(inputs['input_ids'])
            predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Simple exact match evaluation
        if predicted_answer.lower().strip() == gold_answer.lower().strip():
            correct += 1
    
    accuracy = correct / total
    return accuracy

# Test data
test_data = [
    ("What is AI?", "Artificial intelligence is computer science.", "artificial intelligence"),
    ("Who invented the telephone?", "Alexander Graham Bell invented the telephone.", "Alexander Graham Bell"),
    ("What is the capital of France?", "Paris is the capital of France.", "Paris")
]

# Evaluate system
accuracy = evaluate_qa_system(model, tokenizer, test_data)
print(f"QA System Accuracy: {accuracy:.2%}")
``` 