# Question Answering (QA)

Question Answering (QA) is a task in NLP where a system is required to provide answers to questions posed in natural language. Modern QA systems leverage large pre-trained language models and can be categorized as extractive or generative.

## 1. Types of Question Answering

- **Extractive QA:** The answer is a span of text extracted from a given context.
- **Generative QA:** The answer is generated by the model, possibly synthesizing information from multiple sources.

## 2. Extractive QA: Span Prediction

Given a context $`C`$ and a question $`Q`$, the model predicts the start and end positions $`(s, e)`$ of the answer span in $`C`$.

```math
(s^*, e^*) = \underset{(s, e)}{\arg\max}\; P(s, e \mid C, Q)
```

### Example: Using BERT for Extractive QA

```python
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

# Load pre-trained model and tokenizer
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"

inputs = tokenizer(question, context, return_tensors='pt')
outputs = model(**inputs)

answer_start = torch.argmax(outputs.start_logits)
answer_end = torch.argmax(outputs.end_logits) + 1
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))
print("Answer:", answer)
```

## 3. Generative QA

Generative models (e.g., T5, GPT) generate answers in free-form text, not limited to spans in the context.

### Example: Using T5 for Generative QA

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"
input_text = f"question: {question}  context: {context}"
input_ids = tokenizer.encode(input_text, return_tensors='pt')
outputs = model.generate(input_ids)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Answer:", answer)
```

## 4. Applications
- Search engines
- Virtual assistants
- Customer support bots
- Reading comprehension

## 5. Further Reading
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)
- [A Unified Approach to QA (T5, Raffel et al., 2020)](https://arxiv.org/abs/1910.10683) 