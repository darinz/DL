# Question Answering (QA)

Question Answering (QA) is a task in NLP where a system is required to provide answers to questions posed in natural language. Modern QA systems leverage large pre-trained language models and can be categorized as extractive or generative.

---

## 1. Types of Question Answering

- **Extractive QA:** The answer is a span of text extracted from a given context.
- **Generative QA:** The answer is generated by the model, possibly synthesizing information from multiple sources.

> **Note:** Extractive QA is common in reading comprehension datasets (e.g., SQuAD), while generative QA is used in open-domain settings and conversational agents.

---

## 2. Extractive QA: Span Prediction

Given a context $C$ and a question $Q$, the model predicts the start and end positions $(s, e)$ of the answer span in $C$.

```math
(s^*, e^*) = \underset{(s, e)}{\arg\max}\; P(s, e \mid C, Q)
```

**Explanation:**
- The model computes the probability of every possible start and end position in the context.
- The pair $(s^*, e^*)$ with the highest probability is selected as the answer span.
- This approach is used by models like BERT for extractive QA.

### Example: Using BERT for Extractive QA

```python
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

# Load pre-trained model and tokenizer
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"

inputs = tokenizer(question, context, return_tensors='pt')  # Tokenize question and context
outputs = model(**inputs)  # Get start and end logits

answer_start = torch.argmax(outputs.start_logits)  # Most probable start position
answer_end = torch.argmax(outputs.end_logits) + 1  # Most probable end position (+1 for inclusive slicing)
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))  # Convert token IDs to string
print("Answer:", answer)
```

**Code Annotations:**
- The tokenizer encodes both the question and context into input IDs for the model.
- The model outputs logits (scores) for each possible start and end position in the context.
- `torch.argmax` finds the positions with the highest scores.
- The answer is extracted by converting the predicted token IDs back to text.

> **Tip:** For longer contexts, models may have a maximum input length. Consider splitting or truncating long documents.

---

## 3. Generative QA

Generative models (e.g., T5, GPT) generate answers in free-form text, not limited to spans in the context.

**Explanation:**
- The model can synthesize information, paraphrase, or combine facts from multiple sources.
- Useful for open-domain QA and conversational agents.

### Example: Using T5 for Generative QA

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained('t5-base')  # Load T5 model

tokenizer = T5Tokenizer.from_pretrained('t5-base')  # Load tokenizer

context = "Deep learning enables computers to learn from data."
question = "What does deep learning enable?"
input_text = f"question: {question}  context: {context}"
input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Encode input as tensor
outputs = model.generate(input_ids)  # Generate answer
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Decode to text
print("Answer:", answer)
```

**Code Annotations:**
- The input is formatted as a string combining the question and context.
- The tokenizer encodes the input for the model.
- The model generates an answer sequence, which is decoded to text.

> **Tip:** You can use larger models (e.g., T5-large) or fine-tune on your own QA data for better performance.

---

## 4. Applications
- **Search engines:** Answering user queries directly from indexed documents.
- **Virtual assistants:** Providing factual answers and task completion.
- **Customer support bots:** Resolving user issues by extracting or generating answers from documentation.
- **Reading comprehension:** Evaluating understanding of text passages.

---

## 5. Further Reading
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)
- [A Unified Approach to QA (T5, Raffel et al., 2020)](https://arxiv.org/abs/1910.10683) 

> **Explore these papers to learn more about the theory and practice of question answering!** 